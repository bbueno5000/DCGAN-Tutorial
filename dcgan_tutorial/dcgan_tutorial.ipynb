{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCEe+ex9pknTviic+kPtfc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gNWUUlbPOSra"},"outputs":[],"source":["\"\"\"\n","DOCSTRING\n","\"\"\"\n","import glob\n","import imageio\n","import matplotlib.pyplot as pyplot\n","import PIL\n","import tensorflow\n","import time\n","\n","class DCGAN:\n","    \"\"\"\n","    DOCSTRING\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        self.cross_entropy = tensorflow.keras.losses.BinaryCrossentropy(from_logits=True)\n","        self.generator_optimizer = tensorflow.keras.optimizers.Adam(1e-4)\n","        self.discriminator_optimizer = tensorflow.keras.optimizers.Adam(1e-4)\n","        self.discriminator = self.make_discriminator_model()\n","        self.generator = self.make_generator_model()\n","        self.noise_dim = 100\n","\n","    def create_gif(self):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        anim_file = 'dcgan.gif'\n","        with imageio.get_writer(anim_file, mode='I') as writer:\n","          filenames = glob.glob('image*.png')\n","          filenames = sorted(filenames)\n","          for filename in filenames:\n","            image = imageio.imread(filename)\n","            writer.append_data(image)\n","          image = imageio.imread(filename)\n","          writer.append_data(image)\n","\n","    def discriminator_loss(self, real_output, fake_output):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        real_loss = self.cross_entropy(tensorflow.ones_like(real_output), real_output)\n","        fake_loss = self.cross_entropy(tensorflow.zeros_like(fake_output), fake_output)\n","        total_loss = real_loss + fake_loss\n","        return total_loss\n","\n","    def display_image(self, epoch_no):\n","        \"\"\"\n","        This function displays a single image using the epoch number.\n","        \"\"\"\n","        return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n","\n","    def generate_and_save_images(self, model, epoch, test_input):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        predictions = model(test_input, training=False)\n","        fig = pyplot.figure(figsize=(4, 4))\n","        for i in range(predictions.shape[0]):\n","            pyplot.subplot(4, 4, i+1)\n","            pyplot.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n","            pyplot.axis('off')\n","        pyplot.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","        pyplot.show()\n","\n","    def generator_loss(self, fake_output):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        return self.cross_entropy(tensorflow.ones_like(fake_output), fake_output)\n","\n","    def make_discriminator_model(self):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        model = tensorflow.keras.Sequential()\n","        model.add(tensorflow.keras.layers.Conv2D(\n","            64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n","        model.add(tensorflow.keras.layers.LeakyReLU())\n","        model.add(tensorflow.keras.layers.Dropout(0.3))\n","        model.add(tensorflow.keras.layers.Conv2D(\n","            128, (5, 5), strides=(2, 2), padding='same'))\n","        model.add(tensorflow.keras.layers.LeakyReLU())\n","        model.add(tensorflow.keras.layers.Dropout(0.3))\n","        model.add(tensorflow.keras.layers.Flatten())\n","        model.add(tensorflow.keras.layers.Dense(1))\n","        return model\n","\n","    def make_generator_model(self):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        model = tensorflow.keras.Sequential()\n","        model.add(tensorflow.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n","        model.add(tensorflow.keras.layers.BatchNormalization())\n","        model.add(tensorflow.keras.layers.LeakyReLU())\n","        model.add(tensorflow.keras.layers.Reshape((7, 7, 256)))\n","        model.add(tensorflow.keras.layers.Conv2DTranspose(\n","            128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n","        model.add(tensorflow.keras.layers.BatchNormalization())\n","        model.add(tensorflow.keras.layers.LeakyReLU())\n","        model.add(tensorflow.keras.layers.Conv2DTranspose(\n","            64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n","        model.add(tensorflow.keras.layers.BatchNormalization())\n","        model.add(tensorflow.keras.layers.LeakyReLU())\n","        model.add(tensorflow.keras.layers.Conv2DTranspose(\n","            1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","        return model\n","\n","    def train(self, dataset, epochs, batch_size):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        num_examples_to_generate = 16\n","        seed = tensorflow.random.normal([num_examples_to_generate, self.noise_dim])\n","        checkpoint = tensorflow.train.Checkpoint(\n","            generator_optimizer=self.generator_optimizer,\n","            discriminator_optimizer=self.discriminator_optimizer,\n","            generator=self.generator, discriminator=self.discriminator)\n","        for epoch in range(epochs):\n","            start = time.time()\n","            for image_batch in dataset:\n","              self.train_step(image_batch, batch_size)\n","            self.generate_and_save_images(self.generator, epoch + 1, seed)\n","            if (epoch + 1) % 15 == 0:\n","              checkpoint.save(file_prefix='.\\\\training_checkpoints\\\\ckpt')\n","            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","        self.generate_and_save_images(self.generator, epochs, seed)\n","\n","    @tensorflow.function\n","    def train_step(self, images, batch_size):\n","        \"\"\"\n","        DOCSTRING\n","        \"\"\"\n","        print('step')\n","        noise = tensorflow.random.normal([batch_size, self.noise_dim])\n","        with tensorflow.GradientTape() as gen_tape, tensorflow.GradientTape() as disc_tape:\n","          generated_images = self.generator(noise, training=True)\n","          real_output = self.discriminator(images, training=True)\n","          fake_output = self.discriminator(generated_images, training=True)\n","          gen_loss = self.generator_loss(fake_output)\n","          disc_loss = self.discriminator_loss(real_output, fake_output)\n","        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n","        gradients_of_discriminator = disc_tape.gradient(\n","            disc_loss, self.discriminator.trainable_variables)\n","        self.generator_optimizer.apply_gradients(zip(\n","            gradients_of_generator, self.generator.trainable_variables))\n","        self.discriminator_optimizer.apply_gradients(zip(\n","            gradients_of_discriminator, self.discriminator.trainable_variables))\n","\n","if __name__ == '__main__':\n","    (train_images, train_labels), (_, _) = tensorflow.keras.datasets.mnist.load_data()\n","    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n","    train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n","    batch_size = 256\n","    train_dataset = tensorflow.data.Dataset.from_tensor_slices(\n","        train_images).shuffle(60000).batch(batch_size)\n","    dcgan = DCGAN()\n","    dcgan.train(train_dataset, 50, batch_size)"]}]}